{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import time\n",
    "import requests\n",
    "from urllib.parse import urlparse, urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import os\n",
    "import urllib.request\n",
    "from urllib.request import urlopen\n",
    "import pyautogui\n",
    "from requests.exceptions import ConnectionError\n",
    "# from pywebcopy import save_webpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "internal_urls = set()\n",
    "external_urls = set()\n",
    "def is_url_valid(url):\n",
    "   #Checks if the url is valid or not\n",
    "    parsedURL = urlparse(url)\n",
    "    return bool(parsedURL.netloc) and bool(parsedURL.scheme)\n",
    "\n",
    "#The above function works as follows. We check if the scheme is present and there is a value in the network location part\n",
    "# url = \"https://umd.edu/virusinfo\"\n",
    "# urlparse(url)\n",
    "# ParseResult(scheme='https', netloc='umd.edu', path='/virusinfo', \n",
    "# params='', query='', fragment='')\n",
    "\n",
    "#The function gives all urls\n",
    "def get_all_urls(url):\n",
    "    urls = set()\n",
    "    # domain name of the URL without the protocol (umd.edu in this case)\n",
    "    domain_name = urlparse(url).netloc\n",
    "    soup = BeautifulSoup(requests.get(url).content, \"html.parser\")\n",
    "    for a_tag in soup.findAll(\"a\"):\n",
    "        href = a_tag.attrs.get(\"href\")\n",
    "        if href == \"\" or href is None:\n",
    "            #href is empty and we don't need that a element\n",
    "            continue\n",
    "        #if the link is not absolute, make it by joining relative to the base\n",
    "        href = urljoin(url, href)\n",
    "        parsed_href = urlparse(href)\n",
    "        #constructing an absolute URL from parsed data\n",
    "        href = parsed_href.scheme + \"://\" + parsed_href.netloc + parsed_href.path\n",
    "        if not is_url_valid(href):\n",
    "            #in valid url\n",
    "            continue\n",
    "        if href in internal_urls:\n",
    "            #it is already in the set, so we don't need to add\n",
    "            continue\n",
    "        if domain_name not in href:\n",
    "            #it is an external link. i.e\n",
    "            # Check if it is already there \n",
    "            if href not in external_urls:\n",
    "#                 print(f\"[EXT] External link: {href}\")\n",
    "                external_urls.add(href)\n",
    "            continue\n",
    "#         print(\".\",end=\" \")\n",
    "        print(f\"[INT] Internal link: {href}\")\n",
    "\n",
    "        urls.add(href)\n",
    "        internal_urls.add(href)\n",
    "    return urls\n",
    "\n",
    "total_urls_visited = 0\n",
    "def crawl(url, max_urls=50):\n",
    "    #Max URL is just to decrease the time if there are a lot of pages.\n",
    "    #The following code was openly available of github and I found this\n",
    "    #idea useful to inhibit crawling time\n",
    "    global total_urls_visited\n",
    "    total_urls_visited += 1\n",
    "    try:\n",
    "        links = get_all_urls(url)\n",
    "        for link in links:\n",
    "            if total_urls_visited > max_urls:\n",
    "                break\n",
    "            crawl(link, max_urls=max_urls)\n",
    "    except:\n",
    "        print(\"caught\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_urls = [\"https://www.atc.edu/About/News/atc-update-2019-novel-coronavirus-covid-19\",\n",
    "\"https://www.alamancecc.edu/health-alert/\",\n",
    "\"https://www.allencc.edu/~board/general-information/post/covid-19-updates\",\n",
    "\"https://www.altierus.edu/studentcommunications\",\n",
    "\"http://www.angelina.edu/coronavirus/\",\n",
    "\"https://atlantatech.edu/coronavirus-covid%e2%80%9119-advisory/\",\n",
    "\"https://www.augustatech.edu/covid19info2020.cms\",\n",
    "\"https://www.actx.edu/COVID19\",\n",
    "\"https://www.bccc.edu/coronavirus\",\n",
    "\"https://www.mybrcc.edu/coronavirus/\",\n",
    "\"https://www.baycollege.edu/student-life/health-safety/coronavirus.php\"]\n",
    "len(base_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_count=0\n",
    "for base_url in base_urls:\n",
    "    track_count = track_count+1\n",
    "    internal_urls.add(base_url)\n",
    "    parsedurl = urlparse(base_url)\n",
    "    print(parsedurl)\n",
    "    base_url_main = parsedurl.scheme+\"://\"+parsedurl.netloc\n",
    "    print(base_url_main)\n",
    "    # base_url_text = base_url.split(\"//\",1)[1]\n",
    "    base_url_text = parsedurl.netloc+parsedurl.path\n",
    "    print(base_url_text)\n",
    "    # base_url_text_domain = base_url_text.split(\"/\",1)[0]\n",
    "    base_url_text_domain = parsedurl.netloc\n",
    "    print(base_url_text_domain)\n",
    "    crawl(base_url)\n",
    "    print(\"[+] Total External links:\", len(external_urls))\n",
    "    print(\"[+] Total Internal links:\", len(internal_urls))\n",
    "    print(\"[+] Total:\", len(external_urls) + len(internal_urls))\n",
    "    count = 0\n",
    "    last_count = 989874\n",
    "    immediate_urls = set() #Linked Associated with the current page, https://umd.edu/virusinfo/ in this case\n",
    "\n",
    "    for i in range(0,4):\n",
    "        for url in internal_urls:\n",
    "            if base_url_text in url:\n",
    "                immediate_urls.add(url)\n",
    "        count = len(immediate_urls)\n",
    "        print(count,i)\n",
    "        if(count == last_count):\n",
    "            break\n",
    "        last_count = count\n",
    "        for immediate_url in immediate_urls:\n",
    "            try:\n",
    "                crawl(immediate_url)\n",
    "                print(\"[+] NEW Total External links:\", len(external_urls))\n",
    "                print(\"[+] NEW Total Internal links:\", len(internal_urls))\n",
    "                print(\"[+] NEW Total:\", len(external_urls) + len(internal_urls))\n",
    "                if len(internal_urls)>1000:\n",
    "                    break\n",
    "            except requests.exceptions.ConnectionError:\n",
    "                continue\n",
    "\n",
    "    def removeAfterN(yourStr, nth, occurenceOf):\n",
    "        return occurenceOf.join(yourStr.split(occurenceOf)[:nth])\n",
    "    \n",
    "    \n",
    "    semifinal_immediate_urls = set()\n",
    "\n",
    "    for immediate_url in immediate_urls:\n",
    "        stripped_url = removeAfterN(immediate_url,2,\"http\")\n",
    "\n",
    "\n",
    "        semifinal_immediate_urls.add(stripped_url)\n",
    "    #     final_immediate_urls.add(rest)\n",
    "    #     print(rest)\n",
    "\n",
    "\n",
    "\n",
    "    for i in semifinal_immediate_urls:\n",
    "        print(i)\n",
    "    total_count = len(semifinal_immediate_urls)\n",
    "    print(total_count)\n",
    "    \n",
    "    \n",
    "    final_immediate_urls = set()\n",
    "    for semifinal_immediate_url in semifinal_immediate_urls:\n",
    "        if \"tel:\" not in semifinal_immediate_url:\n",
    "            final_immediate_urls.add(semifinal_immediate_url)\n",
    "\n",
    "\n",
    "    for i in final_immediate_urls:\n",
    "        print(i)\n",
    "    total_count = len(final_immediate_urls)\n",
    "    print(total_count)       \n",
    "    \n",
    "    \n",
    "    \n",
    "    parent_folder = \"Associates\" #Change based on folder\n",
    "    name_of_folder = f'{base_url_text_domain}_Screenshots'\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.headless = True\n",
    "    driver = webdriver.Chrome(options=options,executable_path='chromedriver.exe') # #Local Path of chrome driver\n",
    "    if not os.path.exists(f'{parent_folder}\\\\{name_of_folder}'):\n",
    "        os.makedirs(f'{parent_folder}\\\\{name_of_folder}')\n",
    "    \n",
    "    \n",
    "    \n",
    "    webpage_save = set()\n",
    "    webpage_save_count = 0\n",
    "    webpage_cannot_save = 0\n",
    "    for i in final_immediate_urls:\n",
    "        if ' ' in i:\n",
    "            continue\n",
    "        try:\n",
    "            code=urlopen(i).getcode()\n",
    "            webpage_save.add(i)\n",
    "            webpage_save_count = webpage_save_count+1\n",
    "            print(f\"{code} can_save - {i}\")\n",
    "        except (urllib.error.HTTPError,urllib.error.URLError) as e:\n",
    "            print(\"discarded\")\n",
    "            webpage_cannot_save = webpage_cannot_save+1\n",
    "            continue\n",
    "\n",
    "    print(f'Webpages saved as HTML :{webpage_save_count}')\n",
    "    print(f'Webpages unable to save as HTML :{webpage_cannot_save}')\n",
    "\n",
    "    \n",
    "    pdf_count = 0\n",
    "    pdf_urls = set()\n",
    "    for webpage in webpage_save:\n",
    "        try:\n",
    "            if webpage[-4:] == \".pdf\":\n",
    "                url = webpage\n",
    "                file_name = url.replace(base_url,'')\n",
    "                file_name = file_name.replace('/','')\n",
    "                if file_name == '':\n",
    "                    file_name = \"Home\"\n",
    "                pdf_urls.add(webpage)\n",
    "                print(webpage)\n",
    "                r = requests.get(url, stream=True)\n",
    "                with open(f'{parent_folder}\\\\{name_of_folder}\\\\{file_name}.pdf', 'wb') as fd:\n",
    "                    for chunk in r.iter_content(2000):\n",
    "                        fd.write(chunk)\n",
    "                pdf_count = pdf_count + 1\n",
    "        except:\n",
    "            print(\"caught\")\n",
    "\n",
    "    print(f\"Pdfs saved: {pdf_count}\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    final_immediate_urls = final_immediate_urls - pdf_urls\n",
    "    webpage_save = webpage_save - pdf_urls\n",
    "    web_save_count = len(webpage_save)\n",
    "    total_count = len(final_immediate_urls)\n",
    "    print(total_count) \n",
    "    \n",
    "    \n",
    "    \n",
    "    final_webpage_save_count = webpage_save_count - pdf_count\n",
    "    print(final_webpage_save_count)\n",
    "    \n",
    "    \n",
    "    webpage_save_counter_loop = 0\n",
    "    for webpage in webpage_save:\n",
    "        try:\n",
    "            url = webpage\n",
    "            file_name = url.replace(base_url,'')\n",
    "            file_name = file_name.replace('/','')\n",
    "            if file_name == '':\n",
    "                    file_name = \"Home\"\n",
    "            urllib.request.urlretrieve (webpage, f\"{parent_folder}\\\\{name_of_folder}\\\\{file_name}.html\")\n",
    "            webpage_save_counter_loop = webpage_save_counter_loop+1\n",
    "            print(f'{webpage_save_counter_loop} of {final_webpage_save_count} - {webpage}')\n",
    "        except:\n",
    "            print(\"caught\")\n",
    "\n",
    "    print(\"Saving Completed\")\n",
    "    \n",
    "    \n",
    "    count = 0\n",
    "    for i in final_immediate_urls:\n",
    "        try:\n",
    "            url = i\n",
    "            file_name = url.replace(base_url,'')\n",
    "            file_name = file_name.replace('/','')\n",
    "            if file_name == '':\n",
    "                    file_name = \"Home\"\n",
    "            count = count + 1\n",
    "            print(f\"{count} of {total_count}\")\n",
    "            print(f'Visiting {base_url_text}/{file_name}')\n",
    "            print(f'...Taking a screenshot')\n",
    "            driver.get(url)\n",
    "            S = lambda X: driver.execute_script('return document.body.parentNode.scroll'+X)\n",
    "            driver.set_window_size(S('Width'),S('Height')) # May need manual adjustment                                                                                                                \n",
    "            driver.find_element_by_tag_name('body').screenshot(f'{parent_folder}\\\\{name_of_folder}\\\\{file_name}.png')\n",
    "            print(f'Screenshot of {file_name} page taken! \\n')\n",
    "        except:\n",
    "            print(\"caught\")\n",
    "    #     print(\"Saving HTML\")\n",
    "    #     pyautogui.hotkey('ctrl', 's')\n",
    "    #     time.sleep(1)\n",
    "    #     pyautogui.typewrite(f'{file_name}.html')\n",
    "    #     pyautogui.hotkey('enter')\n",
    "    driver.quit()\n",
    "    \n",
    "    print(\"#######################################################################################################################################################\")\n",
    "    \n",
    "    print(f'Task Completed! Files stored in the {parent_folder}\\\\{name_of_folder} Folder')\n",
    "\n",
    "print(len(base_urls))\n",
    "print(track_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
